{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**定义字段选取字典**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from paddle.io import Dataset\r\n",
    "from baseline_tools import Data2IdNorm,Data2IdEmb,value2numpy,make_dict_file\r\n",
    "# None表示不使用，“emb”为Embedding预处理方案\r\n",
    "TAGS = {'android_id': None,\r\n",
    "        'apptype': \"emb\",\r\n",
    "        'carrier':  \"emb\",\r\n",
    "        'dev_height': \"emb\",\r\n",
    "        'dev_ppi': \"emb\",\r\n",
    "        'dev_width': \"emb\",\r\n",
    "        'lan': \"emb\",\r\n",
    "        'media_id': \"emb\",\r\n",
    "        'ntt': \"emb\",\r\n",
    "        'os':\"emb\",\r\n",
    "        'osv': \"emb\",\r\n",
    "        'package': \"emb\",\r\n",
    "        'sid': None,\r\n",
    "        'timestamp': \"norm\",\r\n",
    "        'version': \"emb\",\r\n",
    "        'fea_hash': \"norm\",\r\n",
    "        'location': \"emb\",\r\n",
    "        'fea1_hash': \"norm\",\r\n",
    "        'cus_type': \"emb\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据预处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#我这里是对fea_hash，fea1_hash,timestamp这3个字段用norm处理，可是发现fea_hash数据集中有很多异常值，进行处理一下\r\n",
    "# fea_hash字段中有许多异常数据，将他们改为最大值的一半：499997879.0\r\n",
    "datas = pd.read_csv(\"train.csv\")\r\n",
    "#datas = datas[\"fea_hash\"]\r\n",
    "\r\n",
    "for ids,data in enumerate(datas[\"fea_hash\"]):\r\n",
    "    try:\r\n",
    "        data = float(data)\r\n",
    "    except:\r\n",
    "        datas[\"fea_hash\"][ids] = 499997879\r\n",
    "        print(ids+1)\r\n",
    "datas.to_csv(\"train.csv\")\r\n",
    "#fea_hash字段中有许多异常数据，将他们改为最大值的一半：499997879.0\r\n",
    "datas = pd.read_csv(\"test.csv\",dtype=str)\r\n",
    "#datas = datas[\"fea_hash\"]\r\n",
    "#print(datas.head)\r\n",
    "\r\n",
    "for ids,data in enumerate(datas[\"fea_hash\"]):\r\n",
    "    try:\r\n",
    "        data = float(data)\r\n",
    "    except:\r\n",
    "        datas[\"fea_hash\"][ids] = 499997879\r\n",
    "        print(ids+1)\r\n",
    "datas = datas\r\n",
    "datas.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#生成字典,顺便计算一下norm字段中的最大值，用于归一化\r\n",
    "%mkdir emb_dicts\r\n",
    "TRAIN_PATH = \"train.csv\"\r\n",
    "SAVE_PATH = \"emb_dicts\"\r\n",
    "df = pd.read_csv(TRAIN_PATH, index_col=0)\r\n",
    "# 对两组连续数据取对数，使数据呈正态分布（对比之前直接用效果有略微提升）\r\n",
    "df['fea1_hash'] = np.log(df['fea1_hash'])\r\n",
    "df['fea_hash'] = np.log(df['fea1_hash'])\r\n",
    "\r\n",
    "pack = dict()\r\n",
    "for tag, tag_method in TAGS.items():\r\n",
    "    if tag_method != \"emb\":\r\n",
    "        if tag_method == \"norm\":\r\n",
    "            data = df.loc[:, tag]\r\n",
    "            print(\"{}_max的倒数:{}\".format(tag,1/float(data.max())),\"--------\",float(data.max())/2)\r\n",
    "            print(\"{}_max:{}\".format(tag,float(data.max())),\"--------min:\",float(data.min()))\r\n",
    "            #print(\"{}_mean:{}\".format(tag,data.mean()))\r\n",
    "        continue\r\n",
    "    data = df.loc[:, tag]\r\n",
    "    dict_size = make_dict_file(data, SAVE_PATH, dict_name=tag)\r\n",
    "    pack[tag] = dict_size + 1  # +1是为了增加字典中不存在的情况，提供一个默认值\r\n",
    "\r\n",
    "with open(os.path.join(SAVE_PATH, \"size.dict\"), \"w\", encoding=\"utf-8\") as f:\r\n",
    "    f.write(str(pack))\r\n",
    "\r\n",
    "print(\"全部生成完毕\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将上面计算出的权重结果复制到NORM_WEIGHT中\r\n",
    "NORM_WEIGHT = {'timestamp': 6.409845522722902e-13,\r\n",
    "                \"fea_hash\":0.3226648518870102,\r\n",
    "                \"fea1_hash\":0.045085662640681215,\r\n",
    "                \"android_id\":1.4086530e-06,\r\n",
    "                \"dev_height\":0.00011081560283687943,\r\n",
    "                \"dev_ppi\":0.001388888888888889,\r\n",
    "                \"dev_width\":0.00011322463768115942\r\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据读取方法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_size_dict(dict_path=\"./emb_dicts/size.dict\"):\r\n",
    "    \"\"\"\r\n",
    "    获取Embedding推荐大小\r\n",
    "    :param dict_path: 由run_make_emb_dict.py生成的size.dict\r\n",
    "    :return: 推荐大小字典{key: num}\r\n",
    "    \"\"\"\r\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as f:\r\n",
    "        try:\r\n",
    "            size_dict = eval(f.read())\r\n",
    "        except Exception as e:\r\n",
    "            print(\"size_dict打开失败，请检查\", dict_path, \"文件是否正常，报错信息如下:\\n\", e)\r\n",
    "        return size_dict\r\n",
    "# 定义数据读取方法\r\n",
    "def read_file(use_mini_train, is_infer):\r\n",
    "\r\n",
    "    # 选择文件名\r\n",
    "    emb_dict_path=\"./emb_dicts\"\r\n",
    "    train_name = \"mini_train\" if use_mini_train else \"train\"\r\n",
    "    file_name = \"test\" if is_infer else train_name\r\n",
    "    # 根据文件名读取对应csv文件\r\n",
    "    df = pd.read_csv(file_name + \".csv\")\r\n",
    "    # 对两组连续数据取对数，使数据呈正态分布（对比之前直接用效果有略微提升）\r\n",
    "    df['fea1_hash'] = np.log(df['fea1_hash'])\r\n",
    "    df['fea_hash'] = np.log(df['fea1_hash'])\r\n",
    "\r\n",
    "\r\n",
    "    # 数据预处理\r\n",
    "    cols = [tag for tag, tag_method in TAGS.items() if tag_method is not None]\r\n",
    "    methods = dict()\r\n",
    "    for col in cols:\r\n",
    "        # ===== 预处理方法注册 =====\r\n",
    "        if TAGS[col] == \"emb\":\r\n",
    "            methods[col] = Data2IdEmb(dict_path=emb_dict_path, dict_name=col).get_method()\r\n",
    "        elif TAGS[col] == \"norm\":\r\n",
    "            methods[col] = Data2IdNorm(norm_weight=NORM_WEIGHT[col]).get_method()\r\n",
    "        else:\r\n",
    "            raise Exception(str(TAGS) + \"是未知的预处理方案，请选手在此位置使用elif注册\")\r\n",
    "\r\n",
    "    pack = []\r\n",
    "    \r\n",
    "    # 遍历指定数量的字段\r\n",
    "    for i in df.index:\r\n",
    "        ll = []\r\n",
    "        for col in cols:    \r\n",
    "            sample = df.loc[i, col]\r\n",
    "            sample = methods[col](sample)\r\n",
    "            ll.append(sample)\r\n",
    "            \r\n",
    "        pack.append(ll)\r\n",
    "\r\n",
    "    pack = np.array(pack)\r\n",
    "    # 如果是训练集返回标签和数据，测试集只返回数据\r\n",
    "    if file_name == train_name:\r\n",
    "        labels = []\r\n",
    "        for i in df[\"label\"]:\r\n",
    "            labels.append(i)\r\n",
    "       \r\n",
    "        labels = np.array(labels)\r\n",
    "        return pack, labels, df\r\n",
    "    else:\r\n",
    "        return pack, df\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**训练部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection as ms\r\n",
    "from xgboost import XGBClassifier\r\n",
    "from xgboost import plot_importance\r\n",
    "from matplotlib import pyplot\r\n",
    "import sklearn.metrics as sm\r\n",
    "\r\n",
    "\r\n",
    "# 调了半天参数不如用默认参数\r\n",
    "# model =  XGBClassifier(\r\n",
    "#         booster='gbtree',\r\n",
    "#         eval_metric='auc',\r\n",
    "#         n_estimators=200,\r\n",
    "#         learning_rate =0.07,\r\n",
    "#         max_depth=6,\r\n",
    "#         min_child_weight=1,\r\n",
    "\r\n",
    "#         gamma=0.3,\r\n",
    "#         subsample=0.7,\r\n",
    "#         colsample_bytree=0.7,\r\n",
    "#         colsample_level=0.7,\r\n",
    "#         objective= 'binary:logistic',\r\n",
    "#         nthread=4,\r\n",
    "#         scale_pos_weight=2,\r\n",
    "#         reg_alpha=5.4,\r\n",
    "#         reg_lambda=1,\r\n",
    "#         seed=27,\r\n",
    "#         alpha=0.1,\r\n",
    "#         eta= 0.1, \r\n",
    "#         silent=0)\r\n",
    "model =  XGBClassifier()\r\n",
    "# 获取训练集结果\r\n",
    "train_reader = read_file(use_mini_train=False, is_infer=False)\r\n",
    "# 交叉验证\r\n",
    "bili = int(round(len(train_reader[1]) * 0.95, 0))\r\n",
    "train_data = train_reader[0].squeeze()\r\n",
    "train_lable = train_reader[1]\r\n",
    "eval_set = [(train_data[bili + 1:], train_lable[bili + 1:])]\r\n",
    "# 训练\r\n",
    "model.fit(train_data[:bili], train_lable[:bili], eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\r\n",
    "pred_test_y = model.predict(train_data[bili + 1:])\r\n",
    "# 评估指标报告\r\n",
    "cr = sm.classification_report(train_lable[bili + 1:], pred_test_y)\r\n",
    "print(cr)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**特征重要性可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_importance(model)\r\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**推理部分**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infer_reader, df = read_file(use_mini_train=False, is_infer=True)\r\n",
    "infer_output = model.predict(infer_reader.squeeze())\r\n",
    "result_df = df[\"sid\"]\r\n",
    "result_df = pd.DataFrame({\"sid\": np.array(result_df, dtype=\"int64\"), \"label\": infer_output})\r\n",
    "RESULT_FILE = \"./result1.csv\" \r\n",
    "result_df.to_csv(RESULT_FILE, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
